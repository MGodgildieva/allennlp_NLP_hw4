{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, LabelField, SequenceLabelField\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer, TokenCharactersIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.modules.token_embedders.token_characters_encoder import TokenCharactersEncoder\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import Predictor\n",
    "from allennlp.common.util import JsonDict\n",
    "import glob\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "class RnnLang(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                token_characters: Dict[str, torch.Tensor],\n",
    "                labels: torch.Tensor = None) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings({**tokens, **token_characters})\n",
    "\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "\n",
    "        logits = self.hidden2tag(encoder_out)\n",
    "        output = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            self.accuracy(logits, labels, mask)\n",
    "            output[\"loss\"] = sequence_cross_entropy_with_logits(logits, labels, mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader(DatasetReader):\n",
    "    \n",
    "    def __init__(self , token_indexers: Dict[str, SingleIdTokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or{\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.token_character_indexers = {\"token_characters\": TokenCharactersIndexer()}\n",
    "        self.all_letters = string.ascii_letters + \" .,;'\"\n",
    "        self.category_lines = {}\n",
    "        self.all_categories = []\n",
    "\n",
    "    def text_to_instance(self, names: List[str], categories: List[str] = None) -> Instance:\n",
    "        tokens = [Token(name) for name in names]\n",
    "        token_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": token_field}\n",
    "        token_character_field = TextField(tokens, self.token_character_indexers)\n",
    "        fields[\"token_characters\"] = token_character_field\n",
    "        if categories != None:\n",
    "            label_field = SequenceLabelField(labels=categories, sequence_field = token_field)\n",
    "            fields[\"labels\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    def unicode_to_ascii(self, s: str) -> str:\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "            and c in self.all_letters\n",
    "        )\n",
    "    def readLines(self, filename:str) -> list:\n",
    "        lines = open(filename).read().strip().split('\\n')\n",
    "        return [self.unicode_to_ascii(line) for line in lines]\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        all_filenames = glob.glob(file_path)\n",
    "        name_cats = []\n",
    "        for filename in all_filenames:\n",
    "            category = filename.split('/')[-1].split('.')[0]\n",
    "            lines = self.readLines(filename)\n",
    "            self.category_lines[category] = lines\n",
    "            name_cats.extend([(word, category) for word in lines])\n",
    "        random.shuffle(name_cats)\n",
    "        for i in range(0, len(name_cats), 10):\n",
    "            chunk = name_cats[i:i + 10]\n",
    "            yield self.text_to_instance([pair[0] for pair in chunk], [pair[1] for pair in chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2008it [00:00, 16074.30it/s]\n"
     ]
    }
   ],
   "source": [
    "data = reader.read('../data/names/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2019 16:23:29 - INFO - allennlp.data.vocabulary -   Fitting token dictionary from dataset.\n",
      "100%|██████████| 2008/2008 [00:00<00:00, 6382.49it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary with namespaces:  tokens, Size: 17424 || token_characters, Size: 57 || labels, Size: 18 || Non Padded Namespaces: {'*labels', '*tags'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_DIM = 3\n",
    "CHAR_EMBEDDING_DIM = 3\n",
    "HIDDEN_DIM =6\n",
    "EMBEDDING_DIM = WORD_EMBEDDING_DIM + CHAR_EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_encoder = PytorchSeq2VecWrapper(torch.nn.RNN(CHAR_EMBEDDING_DIM, CHAR_EMBEDDING_DIM, batch_first=True))\n",
    "token_char_embedding = Embedding(num_embeddings=vocab.get_vocab_size('token_characters'),\n",
    "                            embedding_dim=WORD_EMBEDDING_DIM)\n",
    "char_embeddings = TokenCharactersEncoder(token_char_embedding, char_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=WORD_EMBEDDING_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding, \"token_characters\": char_embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RnnLang(word_embeddings, encoder, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2019 16:39:50 - INFO - allennlp.training.trainer -   Beginning training.\n",
      "03/30/2019 16:39:50 - INFO - allennlp.training.trainer -   Epoch 0/14\n",
      "03/30/2019 16:39:50 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.576\n",
      "03/30/2019 16:39:50 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 1.7559 ||: 100%|██████████| 753/753 [00:05<00:00, 140.19it/s]\n",
      "03/30/2019 16:39:55 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.5997 ||: 100%|██████████| 251/251 [00:00<00:00, 408.21it/s]\n",
      "03/30/2019 16:39:56 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:39:56 - INFO - allennlp.training.trainer -   loss          |     1.756  |     1.600\n",
      "03/30/2019 16:39:56 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.576  |       N/A\n",
      "03/30/2019 16:39:56 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:06\n",
      "03/30/2019 16:39:56 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:24\n",
      "03/30/2019 16:39:56 - INFO - allennlp.training.trainer -   Epoch 1/14\n",
      "03/30/2019 16:39:56 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.68\n",
      "03/30/2019 16:39:56 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 1.5087 ||: 100%|██████████| 753/753 [00:05<00:00, 142.44it/s]\n",
      "03/30/2019 16:40:01 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.5159 ||: 100%|██████████| 251/251 [00:00<00:00, 522.92it/s]\n",
      "03/30/2019 16:40:02 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:40:02 - INFO - allennlp.training.trainer -   loss          |     1.509  |     1.516\n",
      "03/30/2019 16:40:02 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.680  |       N/A\n",
      "03/30/2019 16:40:02 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:05\n",
      "03/30/2019 16:40:02 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:17\n",
      "03/30/2019 16:40:02 - INFO - allennlp.training.trainer -   Epoch 2/14\n",
      "03/30/2019 16:40:02 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.688\n",
      "03/30/2019 16:40:02 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 1.4760 ||: 100%|██████████| 753/753 [00:05<00:00, 128.32it/s]\n",
      "03/30/2019 16:40:07 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.5012 ||: 100%|██████████| 251/251 [00:00<00:00, 475.96it/s]\n",
      "03/30/2019 16:40:08 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:40:08 - INFO - allennlp.training.trainer -   loss          |     1.476  |     1.501\n",
      "03/30/2019 16:40:08 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.688  |       N/A\n",
      "03/30/2019 16:40:08 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:06\n",
      "03/30/2019 16:40:08 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:13\n",
      "03/30/2019 16:40:08 - INFO - allennlp.training.trainer -   Epoch 3/14\n",
      "03/30/2019 16:40:08 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.74\n",
      "03/30/2019 16:40:08 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 1.4678 ||: 100%|██████████| 753/753 [00:05<00:00, 127.41it/s]\n",
      "03/30/2019 16:40:14 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.4939 ||: 100%|██████████| 251/251 [00:00<00:00, 500.41it/s]\n",
      "03/30/2019 16:40:14 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:40:14 - INFO - allennlp.training.trainer -   loss          |     1.468  |     1.494\n",
      "03/30/2019 16:40:14 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.740  |       N/A\n",
      "03/30/2019 16:40:14 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:06\n",
      "03/30/2019 16:40:14 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:08\n",
      "03/30/2019 16:40:14 - INFO - allennlp.training.trainer -   Epoch 4/14\n",
      "03/30/2019 16:40:14 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.764\n",
      "03/30/2019 16:40:14 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 1.4558 ||: 100%|██████████| 753/753 [00:05<00:00, 130.70it/s]\n",
      "03/30/2019 16:40:20 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.4839 ||: 100%|██████████| 251/251 [00:00<00:00, 519.34it/s]\n",
      "03/30/2019 16:40:21 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:40:21 - INFO - allennlp.training.trainer -   loss          |     1.456  |     1.484\n",
      "03/30/2019 16:40:21 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.764  |       N/A\n",
      "03/30/2019 16:40:21 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:06\n",
      "03/30/2019 16:40:21 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:02\n",
      "03/30/2019 16:40:21 - INFO - allennlp.training.trainer -   Epoch 5/14\n",
      "03/30/2019 16:40:21 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.768\n",
      "03/30/2019 16:40:21 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 1.4367 ||: 100%|██████████| 753/753 [00:07<00:00, 106.49it/s]\n",
      "03/30/2019 16:40:28 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.4799 ||: 100%|██████████| 251/251 [00:00<00:00, 350.75it/s]\n",
      "03/30/2019 16:40:28 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:40:28 - INFO - allennlp.training.trainer -   loss          |     1.437  |     1.480\n",
      "03/30/2019 16:40:28 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.768  |       N/A\n",
      "03/30/2019 16:40:28 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:07\n",
      "03/30/2019 16:40:28 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:58\n",
      "03/30/2019 16:40:28 - INFO - allennlp.training.trainer -   Epoch 6/14\n",
      "03/30/2019 16:40:29 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.772\n",
      "03/30/2019 16:40:29 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 1.3612 ||: 100%|██████████| 753/753 [00:05<00:00, 128.87it/s]\n",
      "03/30/2019 16:40:34 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.3739 ||: 100%|██████████| 251/251 [00:00<00:00, 503.56it/s]\n",
      "03/30/2019 16:40:35 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:40:35 - INFO - allennlp.training.trainer -   loss          |     1.361  |     1.374\n",
      "03/30/2019 16:40:35 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.772  |       N/A\n",
      "03/30/2019 16:40:35 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:06\n",
      "03/30/2019 16:40:35 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:51\n",
      "03/30/2019 16:40:35 - INFO - allennlp.training.trainer -   Epoch 7/14\n",
      "03/30/2019 16:40:35 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.772\n",
      "03/30/2019 16:40:35 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 1.2738 ||: 100%|██████████| 753/753 [00:06<00:00, 124.17it/s]\n",
      "03/30/2019 16:40:41 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.2886 ||: 100%|██████████| 251/251 [00:00<00:00, 453.96it/s]\n",
      "03/30/2019 16:40:42 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:40:42 - INFO - allennlp.training.trainer -   loss          |     1.274  |     1.289\n",
      "03/30/2019 16:40:42 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.772  |       N/A\n",
      "03/30/2019 16:40:42 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:06\n",
      "03/30/2019 16:40:42 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:45\n",
      "03/30/2019 16:40:42 - INFO - allennlp.training.trainer -   Epoch 8/14\n",
      "03/30/2019 16:40:42 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.772\n",
      "03/30/2019 16:40:42 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 1.1883 ||: 100%|██████████| 753/753 [00:06<00:00, 120.54it/s]\n",
      "03/30/2019 16:40:48 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.2014 ||: 100%|██████████| 251/251 [00:00<00:00, 456.40it/s]\n",
      "03/30/2019 16:40:48 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:40:48 - INFO - allennlp.training.trainer -   loss          |     1.188  |     1.201\n",
      "03/30/2019 16:40:48 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.772  |       N/A\n",
      "03/30/2019 16:40:48 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/30/2019 16:40:48 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:39\n",
      "03/30/2019 16:40:48 - INFO - allennlp.training.trainer -   Epoch 9/14\n",
      "03/30/2019 16:40:48 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.772\n",
      "03/30/2019 16:40:48 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 1.0782 ||: 100%|██████████| 753/753 [00:05<00:00, 136.60it/s]\n",
      "03/30/2019 16:40:54 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.1071 ||: 100%|██████████| 251/251 [00:00<00:00, 346.59it/s]\n",
      "03/30/2019 16:40:55 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:40:55 - INFO - allennlp.training.trainer -   loss          |     1.078  |     1.107\n",
      "03/30/2019 16:40:55 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.772  |       N/A\n",
      "03/30/2019 16:40:55 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:06\n",
      "03/30/2019 16:40:55 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:32\n",
      "03/30/2019 16:40:55 - INFO - allennlp.training.trainer -   Epoch 10/14\n",
      "03/30/2019 16:40:55 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.772\n",
      "03/30/2019 16:40:55 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 0.9850 ||: 100%|██████████| 753/753 [00:06<00:00, 111.16it/s]\n",
      "03/30/2019 16:41:01 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.0737 ||: 100%|██████████| 251/251 [00:00<00:00, 428.83it/s]\n",
      "03/30/2019 16:41:02 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:41:02 - INFO - allennlp.training.trainer -   loss          |     0.985  |     1.074\n",
      "03/30/2019 16:41:02 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.772  |       N/A\n",
      "03/30/2019 16:41:02 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:07\n",
      "03/30/2019 16:41:02 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:26\n",
      "03/30/2019 16:41:02 - INFO - allennlp.training.trainer -   Epoch 11/14\n",
      "03/30/2019 16:41:02 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.776\n",
      "03/30/2019 16:41:02 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 0.9076 ||: 100%|██████████| 753/753 [00:05<00:00, 127.71it/s]\n",
      "03/30/2019 16:41:08 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.0754 ||: 100%|██████████| 251/251 [00:00<00:00, 483.83it/s]\n",
      "03/30/2019 16:41:08 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:41:08 - INFO - allennlp.training.trainer -   loss          |     0.908  |     1.075\n",
      "03/30/2019 16:41:08 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.776  |       N/A\n",
      "03/30/2019 16:41:08 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:06\n",
      "03/30/2019 16:41:08 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:19\n",
      "03/30/2019 16:41:08 - INFO - allennlp.training.trainer -   Epoch 12/14\n",
      "03/30/2019 16:41:08 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.776\n",
      "03/30/2019 16:41:08 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 0.8314 ||: 100%|██████████| 753/753 [00:05<00:00, 127.11it/s]\n",
      "03/30/2019 16:41:14 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.0887 ||: 100%|██████████| 251/251 [00:00<00:00, 475.41it/s]\n",
      "03/30/2019 16:41:15 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:41:15 - INFO - allennlp.training.trainer -   loss          |     0.831  |     1.089\n",
      "03/30/2019 16:41:15 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.776  |       N/A\n",
      "03/30/2019 16:41:15 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:06\n",
      "03/30/2019 16:41:15 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:13\n",
      "03/30/2019 16:41:15 - INFO - allennlp.training.trainer -   Epoch 13/14\n",
      "03/30/2019 16:41:15 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.776\n",
      "03/30/2019 16:41:15 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 0.7738 ||: 100%|██████████| 753/753 [00:06<00:00, 123.25it/s]\n",
      "03/30/2019 16:41:21 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.0898 ||: 100%|██████████| 251/251 [00:00<00:00, 478.11it/s]\n",
      "03/30/2019 16:41:22 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:41:22 - INFO - allennlp.training.trainer -   loss          |     0.774  |     1.090\n",
      "03/30/2019 16:41:22 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.776  |       N/A\n",
      "03/30/2019 16:41:22 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:06\n",
      "03/30/2019 16:41:22 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:06\n",
      "03/30/2019 16:41:22 - INFO - allennlp.training.trainer -   Epoch 14/14\n",
      "03/30/2019 16:41:22 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 309.776\n",
      "03/30/2019 16:41:22 - INFO - allennlp.training.trainer -   Training\n",
      "loss: 0.7357 ||: 100%|██████████| 753/753 [00:06<00:00, 112.06it/s]\n",
      "03/30/2019 16:41:28 - INFO - allennlp.training.trainer -   Validating\n",
      "loss: 1.1204 ||: 100%|██████████| 251/251 [00:00<00:00, 432.40it/s]\n",
      "03/30/2019 16:41:29 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "03/30/2019 16:41:29 - INFO - allennlp.training.trainer -   loss          |     0.736  |     1.120\n",
      "03/30/2019 16:41:29 - INFO - allennlp.training.trainer -   cpu_memory_MB |   309.776  |       N/A\n",
      "03/30/2019 16:41:29 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'peak_cpu_memory_MB': 309.776,\n",
       " 'training_duration': '00:01:39',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 14,\n",
       " 'epoch': 14,\n",
       " 'training_loss': 0.7357016288703815,\n",
       " 'training_cpu_memory_MB': 309.776,\n",
       " 'validation_loss': 1.1203631451167908,\n",
       " 'best_epoch': 10,\n",
       " 'best_validation_loss': 1.0736781919145013}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"tokens\", \"num_tokens\"), (\"token_characters\", \"num_token_characters\")])\n",
    "iterator.index_with(vocab)\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=training_set,\n",
    "                  validation_dataset=validation_set,\n",
    "                  patience=10,\n",
    "                  num_epochs=15, cuda_device=-1)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LngPredictor(Predictor):\n",
    "    \n",
    "    def predict_json(self, inputs: JsonDict) -> JsonDict:\n",
    "        instance = self._dataset_reader.text_to_instance(inputs)\n",
    "        output_dict = self.predict_instance(instance)\n",
    "        tag_ids = np.argmax(output_dict['logits'], axis=-1)\n",
    "        return [self._model.vocab.get_token_from_index(i, 'labels') for i in tag_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = LngPredictor(model, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict_json([\"Ivanov\", \"Smith\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Russian', 'German']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
